# Assignment 1: Understanding RAG & Embeddings  

## 1. Evolution of Embeddings  
Briefly describe the key limitation of TF-IDF that Word2Vec aimed to solve. What limitation of sequential models (like LSTMs) did the Transformer architecture address with its attention mechanism?

The key limitation of TF-IDF that Word2Vec aimed to solve was that it could only measure word frequency. It could not capture semantic relationships that words had. LSTMs had problems with dependencies that were long range. The solution was to use self attention to analyze words better and quicker.
## 2. BERT's Bidirectional Breakthrough  

BERT's Biderectional Breakthrough was look at all words simultaneously. This allowed for it to understand words from the context of the whole sentence. 

## 3. BERT vs. OpenAI Ada-002  
BERT had privacy and cheap long term use. It was local, free, but slow.

Ada-002 was used for its simplicity and much faster results.It costed money and shared data. 
## 4. Textbook Chunking  
Chunking is necessary as it helps the AI take in those smaller clusters and keeps ideas together.CHunking is crucial as models have strict input limits.

## 5. Model Selection  
1. OpenAI Ada-002 because it is quickt to setup and it is public data 

2. SBERT due to the personal data that the hospital holds of the patients

3.SBERT due to personal data and no costs for API.